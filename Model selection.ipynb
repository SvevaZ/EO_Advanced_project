{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ee\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fcd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate and initialize the Earth Engine API\n",
    "ee.Authenticate()\n",
    "ee.Initialize(project='svevazanettieo1') #così apro il progetto esistente in GEE, non ne sto creando uno nuovo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad639be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert Earth Engine FeatureCollection to Pandas DataFrame, eliminating the random column\n",
    "def from_FeatureCollection_to_df(path):\n",
    "    points = ee.FeatureCollection(path)\n",
    "    points_ft = points.getInfo()['features']\n",
    "    points_df = pd.json_normalize(points_ft)\n",
    "    points_df = points_df.drop(columns=['properties.random'], errors='ignore')\n",
    "    return points_df\n",
    "\n",
    "training_points_df = from_FeatureCollection_to_df('projects/svevazanettieo1/assets/Dubai/Dubai_training_indexes')\n",
    "validation_points_df = from_FeatureCollection_to_df('projects/svevazanettieo1/assets/Dubai/Dubai_validation_indexes')\n",
    "\n",
    "print('Dataset size: \\n')\n",
    "print('Training set:', training_points_df.shape)\n",
    "print('Validation set:', validation_points_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature columns (e.g., 'properties.B1', 'properties.B2', etc.)\n",
    "feature_columns = [col for col in training_points_df.columns if col.startswith('properties.B') or col.startswith('properties.n')]\n",
    "feature_columns\n",
    "\n",
    "# Extract features and labels for training\n",
    "X_train = training_points_df[feature_columns]\n",
    "y_train = training_points_df['properties.LC']\n",
    "\n",
    "# Extract features and labels for validation\n",
    "X_val = validation_points_df[feature_columns]\n",
    "y_val = validation_points_df['properties.LC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "joblib.dump(scaler, \"scaler.save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = len(y_train.unique())\n",
    "y_train = to_categorical(y_train, num_classes=N_CLASSES)\n",
    "y_val = to_categorical(y_val, num_classes=N_CLASSES)\n",
    "\n",
    "#qui vogliamo mischiare l'ordine dei dati, altrimenti tutti i primi sono dello stesso tipo, e così via\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "# Shuffle and batch the datasets\n",
    "train_dataset_batch = train_dataset.shuffle(buffer_size=64).batch(8)\n",
    "val_dataset_batch = val_dataset.batch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077e083",
   "metadata": {},
   "source": [
    "# ANN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(N_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with the specified loss function.\n",
    "model1.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fitting the model\n",
    "history1 = model1.fit(train_dataset_batch, validation_data=val_dataset_batch, epochs=100)\n",
    "# Print model summary and save the model\n",
    "model1.summary()\n",
    "model1.save('model1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a037fb",
   "metadata": {},
   "source": [
    "# ANN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d52d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # metric to monitor\n",
    "    patience=15,          \n",
    "    restore_best_weights=True  \n",
    ")\n",
    "\n",
    "# Define the callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',     # Watch validation loss\n",
    "    factor=0.1,             # Drop LR by an order of magnitude (×0.1)\n",
    "    patience=5,             # Wait 5 epochs with no improvement\n",
    "    min_lr=1e-5,            # Lower bound on learning rate\n",
    "    verbose=1\n",
    ")\n",
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(N_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with the specified loss function.\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fitting the model\n",
    "history2 = model2.fit(train_dataset_batch, validation_data=val_dataset_batch, epochs=250, callbacks=[early_stopping, reduce_lr])\n",
    "# Print model summary and save the model\n",
    "model2.summary()\n",
    "model2.save('model2_indexes.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6247c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_accuracy_graph(train_loss, val_loss, train_acc, val_acc):\n",
    "    # Create a figure with two vertically stacked subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(15, 6), sharex=True)\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    ax1.plot(train_loss, label='Training loss', alpha=.8)\n",
    "    ax1.plot(val_loss, label='Validation loss', alpha=.8)\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=.3)\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    ax2.plot(train_acc, label='Training accuracy', alpha=.8)\n",
    "    ax2.plot(val_acc, label='Validation accuracy', alpha=.8)\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=.3)\n",
    "\n",
    "    # Adjust the layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.85)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7968cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and validation accuracy and loss\n",
    "train_acc_1 = history1.history['accuracy']\n",
    "\n",
    "val_acc_1 = history1.history['val_accuracy']\n",
    "\n",
    "train_loss_1 = history1.history['loss']\n",
    "\n",
    "val_loss_1 = history1.history['val_loss']\n",
    "\n",
    "loss_accuracy_graph(train_loss_1, val_loss_1, train_acc_1, val_acc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167cf5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and validation accuracy and loss\n",
    "train_acc_2 = history2.history['accuracy']\n",
    "\n",
    "val_acc_2 = history2.history['val_accuracy']\n",
    "\n",
    "train_loss_2 = history2.history['loss']\n",
    "\n",
    "val_loss_2 = history2.history['val_loss']\n",
    "\n",
    "loss_accuracy_graph(train_loss_2, val_loss_2, train_acc_2, val_acc_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63614554",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_int = np.argmax(y_val, axis=1)\n",
    "\n",
    "y_pred_ANN1 = model1.predict(X_val)\n",
    "y_pred_ANN1 = np.argmax(y_pred_ANN1, axis=1)\n",
    "val_accuracy_ANN1 = accuracy_score(y_val_int, y_pred_ANN1)\n",
    "cm_ANN1 = confusion_matrix(y_val_int, y_pred_ANN1)\n",
    "\n",
    "y_pred_ANN2 = model2.predict(X_val)\n",
    "y_pred_ANN2 = np.argmax(y_pred_ANN2, axis=1)\n",
    "val_accuracy_ANN2 = accuracy_score(y_val_int, y_pred_ANN2)\n",
    "cm_ANN2 = confusion_matrix(y_val_int, y_pred_ANN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b06e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model1\n",
    "del history1\n",
    "del model2\n",
    "del history2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e30b96",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632cbd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "# Initialize an SVM with RBF kernel\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "# Fit model\n",
    "svm.fit(X_shuffled, y_shuffled)\n",
    "\n",
    "# Predictions\n",
    "y_pred_SVM = svm.predict(X_shuffled_val)\n",
    "cm_SVM = confusion_matrix(y_shuffled_val, y_pred_SVM)\n",
    "val_accuracy_SVM = accuracy_score(y_shuffled_val, y_pred_SVM)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Validation Accuracy:\", val_accuracy_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bc5c5",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "# Hyperparameters\n",
    "n_estimators = 100\n",
    "max_depth = 10\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, oob_score=True)\n",
    "\n",
    "clf_balanced = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    class_weight='balanced'  # example for 5 classes\n",
    ")\n",
    "\n",
    "X_shuffled, y_shuffled = shuffle(X_train, y_train, random_state=42)\n",
    "X_shuffled_val, y_shuffled_val = shuffle(X_val, y_val, random_state=42)\n",
    "\n",
    "y_shuffled = np.argmax(y_shuffled, axis=1)\n",
    "y_shuffled_val = np.argmax(y_shuffled_val, axis=1)\n",
    "\n",
    "clf.fit(X_shuffled, y_shuffled)\n",
    "clf_balanced.fit(X_shuffled, y_shuffled)\n",
    "\n",
    "y_pred_RF = clf.predict(X_shuffled_val)\n",
    "\n",
    "y_pred_RF_balanced = clf_balanced.predict(X_shuffled_val)\n",
    "\n",
    "val_accuracy_RF = accuracy_score(y_shuffled_val, y_pred_RF)\n",
    "cm_RF = confusion_matrix(y_shuffled_val, y_pred_RF)\n",
    "\n",
    "val_accuracy_RF_balanced = accuracy_score(y_shuffled_val, y_pred_RF_balanced)\n",
    "cm_RF_balanced = confusion_matrix(y_shuffled_val, y_pred_RF_balanced)\n",
    "\n",
    "print(\"Validation Accuracy RF:\", val_accuracy_RF)\n",
    "print(\"Validation Accuracy RF (Balanced):\", val_accuracy_RF_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533924a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have 4 confusion matrices\n",
    "cm1 = cm_RF\n",
    "cm2 = cm_RF_balanced\n",
    "\n",
    "cms = [cm1, cm2]\n",
    "titles = ['Random Forest', 'Random Forest (Balanced)']\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))  # 2x2 grid\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cms[i], display_labels=['vegetation','water','soil','urban'])\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\", ax=ax, colorbar=False)  # use ax\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2886e06",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = cm_ANN1\n",
    "cm2 = cm_ANN2\n",
    "cm3 = cm_SVM\n",
    "cm4 = cm_RF\n",
    "\n",
    "cms = [cm1, cm2, cm3, cm4]\n",
    "titles = ['ANN1', 'ANN2', 'SVM', 'Random Forest']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # 2x2 grid\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cms[i], display_labels=['vegetation','water','soil','urban'])\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\", ax=ax, colorbar=False)  # use ax\n",
    "    ax.set_title(titles[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30929630",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\")\n",
    "print(\"ANN1:\", val_accuracy_ANN1)\n",
    "print(\"ANN2:\", val_accuracy_ANN2)\n",
    "print(\"SVM:\", val_accuracy_SVM)\n",
    "print(\"Random Forest:\", val_accuracy_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20c394",
   "metadata": {},
   "source": [
    "# Tuning Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cls = np.argmax(y_train, axis=1)\n",
    "y_val_cls   = np.argmax(y_val, axis=1)\n",
    "\n",
    "X = np.vstack((X_train, X_val))\n",
    "y = np.hstack((y_train_cls, y_val_cls))\n",
    "\n",
    "X_shuffled, y_shuffled = shuffle(X, y, random_state=42)\n",
    "\n",
    "crossvalidation = StratifiedKFold(n_splits=30, shuffle=True)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf_1 = RandomForestClassifier(n_estimators=50, max_depth=10, oob_score=True)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "xval_score = cross_val_score(clf_1, X_shuffled, y_shuffled, cv=crossvalidation)\n",
    "\n",
    "# Compute the basic statistics\n",
    "accuracy_mean = np.average(xval_score)\n",
    "accuracy_std = np.std(xval_score)\n",
    "\n",
    "\n",
    "print('Random Forest 1')  # Print method name\n",
    "print(\"========================================\")\n",
    "print(f\"Accuracy (CV): {accuracy_mean:.3f} +/- {accuracy_std:.3f}\")\n",
    "\n",
    "#Second model \n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf_1 = RandomForestClassifier(n_estimators=100, max_depth=10, oob_score=True)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "xval_score = cross_val_score(clf_1, X_shuffled, y_shuffled, cv=crossvalidation)\n",
    "\n",
    "# Compute the basic statistics\n",
    "accuracy_mean = np.average(xval_score)\n",
    "accuracy_std = np.std(xval_score)\n",
    "\n",
    "\n",
    "print('Random Forest 2')  # Print method name\n",
    "print(\"========================================\")\n",
    "print(f\"Accuracy (CV): {accuracy_mean:.3f} +/- {accuracy_std:.3f}\")\n",
    "\n",
    "#third model\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf_1 = RandomForestClassifier(n_estimators=200, max_depth=10, oob_score=True)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "xval_score = cross_val_score(clf_1, X_shuffled, y_shuffled, cv=crossvalidation)\n",
    "\n",
    "# Compute the basic statistics\n",
    "accuracy_mean = np.average(xval_score)\n",
    "accuracy_std = np.std(xval_score)\n",
    "\n",
    "\n",
    "print('Random Forest 3')  # Print method name\n",
    "print(\"========================================\")\n",
    "print(f\"Accuracy (CV): {accuracy_mean:.3f} +/- {accuracy_std:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest classifier\n",
    "clf_1 = RandomForestClassifier(n_estimators=200, max_depth=5, oob_score=True)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "xval_score = cross_val_score(clf_1, X_shuffled, y_shuffled, cv=crossvalidation)\n",
    "\n",
    "# Compute the basic statistics\n",
    "accuracy_mean = np.average(xval_score)\n",
    "accuracy_std = np.std(xval_score)\n",
    "\n",
    "print('Random Forest 1')  # Print method name\n",
    "print(\"========================================\")\n",
    "print(f\"Accuracy (CV): {accuracy_mean:.3f} +/- {accuracy_std:.3f}\")\n",
    "\n",
    "#Second model \n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf_1 = RandomForestClassifier(n_estimators=200, max_depth=10, oob_score=True)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "xval_score = cross_val_score(clf_1, X_shuffled, y_shuffled, cv=crossvalidation)\n",
    "\n",
    "# Compute the basic statistics\n",
    "accuracy_mean = np.average(xval_score)\n",
    "accuracy_std = np.std(xval_score)\n",
    "\n",
    "\n",
    "print('Random Forest 2')  # Print method name\n",
    "print(\"========================================\")\n",
    "print(f\"Accuracy (CV): {accuracy_mean:.3f} +/- {accuracy_std:.3f}\")\n",
    "\n",
    "#third model\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf_1 = RandomForestClassifier(n_estimators=200, max_depth=15, oob_score=True)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "xval_score = cross_val_score(clf_1, X_shuffled, y_shuffled, cv=crossvalidation)\n",
    "\n",
    "# Compute the basic statistics\n",
    "accuracy_mean = np.average(xval_score)\n",
    "accuracy_std = np.std(xval_score)\n",
    "\n",
    "\n",
    "print('Random Forest 3')  # Print method name\n",
    "print(\"========================================\")\n",
    "print(f\"Accuracy (CV): {accuracy_mean:.3f} +/- {accuracy_std:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Hyperparameters\n",
    "n_estimators = 200\n",
    "max_depth = 10\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, oob_score=True)\n",
    "clf.fit(X_shuffled, y_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ace7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(clf, \"random_forest_model_train+val.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
